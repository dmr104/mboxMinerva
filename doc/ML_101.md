# Machine Learning 101: CPT/RAG Email Pipeline with CI Orchestration

## Overview

You're building a **Continual Pre-Training (CPT) pipeline with Retrieval-Augmented Generation (RAG)** to fine-tune a language model on your 45k-email corpus. The pipeline uses:

- **LoRA (Low-Rank Adaptation)**: Efficient fine-tuning that trains small adapter weights instead of the full model
- **Stratified Replay Sampling**: Balances old + new data to prevent catastrophic forgetting
- **Rolling Window**: Incrementally processes new emails while rehearsing strategically sampled past data
- **Deterministic Reproducibility**: SHA256-seeded RNG ensures identical training runs given same inputs

---

## Script Roles (in Pipeline Order)

### 1. **splitter.rb** — Train/Val/Test Split Assignment
**Purpose**: Deterministically assigns each email chunk to train/val/test splits.

**What it does**:
- Reads raw email chunks (with thread_id, sender, length metadata)
- Uses SHA256(chunk_id) to assign 80% train, 10% val, 10% test
- Writes `assignments.json` (immutable split ground truth)
- Writes `length_metadata.json` and `window_idx` for rolling window tracking

**When it runs**: 
- **CI Trigger**: On new email ingestion (daily/weekly schedule, or manual dispatch)
- **Mode**: Append-only (never rewrites existing assignments)

**Example CI job**:
```yaml
ingest-and-split:
  runs-on: ubuntu-latest
  schedule:
    - cron: '0 2 * * *'  # Daily at 2am
  steps:
    - name: Ingest new emails
      run: ./scripts/ingest_emails.sh
    - name: Split new chunks
      run: ruby splitter.rb --input-dir ./email_chunks --output assignments.json --append
    - name: Upload assignments
      uses: actions/upload-artifact@v3
      with:
        name: assignments
        path: assignments.json
```

---

### 2. **manifest_builder.rb** — Rehearsal Manifest Generation
**Purpose**: Creates a stratified, capped list of past-train chunks for replay.

**What it does**:
- Reads `assignments.json` + `length_metadata.json`
- Filters train IDs from shards **outside current rolling window**
- Applies length bucketing (e.g., <512, 512-1024, 1024-2048, 2048+)
- Stratifies by sender/thread_id within each bucket
- Caps per-bucket counts to prevent dominance
- Outputs `old_shards_train.json` (ephemeral, regenerable)

**When it runs**:
- **CI Trigger**: Before each training run (after window advancement or on retrain schedule)
- **Mode**: Regenerates from scratch each time (deterministic via --seed)

**Example CI job**:
```yaml
generate-rehearsal-manifest:
  needs: ingest-and-split
  runs-on: ubuntu-latest
  steps:
    - name: Download assignments
      uses: actions/download-artifact@v3
      with:
        name: assignments
    - name: Generate rehearsal manifest
      run: |
        ruby manifest_builder.rb \
          --assignments assignments.json \
          --length-metadata length_metadata.json \
          --window-idx 5 \
          --seed 1234 \
          --caps '{"short":500,"medium":300,"long":200,"verylong":100}' \
          --output old_shards_train.json
    - name: Upload manifest
      uses: actions/upload-artifact@v3
      with:
        name: rehearsal-manifest
        path: old_shards_train.json
```

---

### 3. **sampler.rb** (StratifiedReplaySampler) — Epoch Schedule Generation
**Purpose**: Creates deterministic training schedules for each epoch.

**What it does**:
- Reads `old_shards_train.json` (past data) + current window train IDs
- Performs two-phase stratified sampling per epoch:
  1. **Min-per-bucket guarantee**: ensures rare strata appear
  2. **Proportional fill**: allocates remaining slots by stratum size
- Interleaves rehearsal:new chunks per epoch schedule (e.g., 30% rehearsal, 70% new)
- Shuffles deterministically using SHA256(seed:epoch:context)
- Outputs per-epoch schedules (list of chunk_ids in training order)

**When it runs**:
- **CI Trigger**: Called internally by `retrain.rb` during training orchestration
- **Mode**: CLI can also pre-generate schedules for inspection

**Example CI job** (if pre-generating):
```yaml
generate-epoch-schedules:
  needs: generate-rehearsal-manifest
  runs-on: ubuntu-latest
  steps:
    - name: Download artifacts
      uses: actions/download-artifact@v3
    - name: Generate schedules
      run: |
        ruby sampler.rb \
          --assignments assignments.json \
          --rehearsal-manifest old_shards_train.json \
          --length-metadata length_metadata.json \
          --window-idx 5 \
          --epochs 3 \
          --seed 42 \
          --rehearsal-ratio 0.3 \
          --output-dir ./schedules/
```

---

### 4. **dataloader.rb** — Batch Iterator
**Purpose**: Yields training batches from epoch schedules.

**What it does**:
- Reads an epoch schedule (list of chunk_ids)
- Groups chunks into batches of size `--batch-size`
- Optionally shuffles within batches (deterministically)
- Yields batches to trainer

**When it runs**:
- **CI Trigger**: Called as library by `retrain.rb` during training loop
- **Mode**: Not typically invoked standalone in CI

---

### 5. **trainer.rb** — Training Loop
**Purpose**: Minimal training loop scaffold with checkpointing.

**What it does**:
- Accepts batches from DataLoader
- Runs forward/backward passes (stub in current implementation)
- Logs progress (loss, throughput)
- Saves checkpoints every N steps
- Supports custom training logic via blocks

**When it runs**:
- **CI Trigger**: Called as library by `retrain.rb`
- **Mode**: Not invoked standalone in CI

---

### 6. **retrain.rb** — Training Orchestrator
**Purpose**: Wires together sampler → dataloader → trainer for end-to-end training.

**What it does**:
- Instantiates StratifiedReplaySampler (sampler.rb)
- Instantiates DataLoader (dataloader.rb)
- Instantiates Trainer (trainer.rb)
- Runs training loop over all epochs
- Saves LoRA checkpoints to `./checkpoints/`

**When it runs**:
- **CI Trigger**: On retrain schedule (weekly, or on window advancement)
- **Mode**: Main training orchestration job

**Example CI job**:
```yaml
train-lora:
  needs: generate-rehearsal-manifest
  runs-on: ubuntu-latest-gpu  # Requires GPU runner
  steps:
    - name: Download artifacts
      uses: actions/download-artifact@v3
    - name: Setup Python/PyTorch
      run: |
        pip install torch transformers peft accelerate
    - name: Run training
      run: |
        ruby retrain.rb \
          --assignments assignments.json \
          --rehearsal-manifest old_shards_train.json \
          --length-metadata length_metadata.json \
          --window-idx 5 \
          --epochs 3 \
          --seed 42 \
          --rehearsal-ratio 0.3 \
          --batch-size 8 \
          --learning-rate 1e-4 \
          --checkpoint-dir ./checkpoints/
    - name: Upload checkpoints
      uses: actions/upload-artifact@v3
      with:
        name: lora-checkpoints
        path: ./checkpoints/
```

---

### 7. **RAG_evaluator.rb** — Checkpoint Evaluation
**Purpose**: Measures perplexity and RAG@K accuracy for each checkpoint.

**What it does**:
- Loads each LoRA checkpoint from `./checkpoints/`
- Runs inference on validation/test sets
- Computes:
  - **Perplexity**: How well the model predicts next tokens (lower = better)
  - **RAG@K**: Top-K retrieval accuracy (does retrieved context help?)
  - **Recall**: Did the model retrieve relevant chunks?
- Writes `eval_results.json` with per-checkpoint metrics

**When it runs**:
- **CI Trigger**: Immediately after training job completes
- **Mode**: Batch evaluation of all checkpoints

**Example CI job**:
```yaml
evaluate-checkpoints:
  needs: train-lora
  runs-on: ubuntu-latest-gpu
  steps:
    - name: Download checkpoints
      uses: actions/download-artifact@v3
      with:
        name: lora-checkpoints
    - name: Evaluate all checkpoints
      run: |
        ruby RAG_evaluator.rb \
          --checkpoint-dir ./checkpoints/ \
          --assignments assignments.json \
          --test-manifest test_chunks.json \
          --output eval_results.json
    - name: Upload evaluation results
      uses: actions/upload-artifact@v3
      with:
        name: eval-results
        path: eval_results.json
```

---

### 8. **lora_checkpoint_selector.rb** — Best Checkpoint Selection
**Purpose**: Picks the best checkpoint via composite scoring.

**What it does**:
- Reads `eval_results.json`
- Applies weighted multi-objective scoring:
  - Example: `score = -0.4*perplexity + 0.6*RAG@5`
- Writes `best_checkpoint.txt` with winning checkpoint name
- Optionally creates symlink or triggers deployment

**When it runs**:
- **CI Trigger**: After evaluation completes
- **Mode**: Final selection step before deployment

**Example CI job**:
```yaml
select-best-checkpoint:
  needs: evaluate-checkpoints
  runs-on: ubuntu-latest
  steps:
    - name: Download evaluation results
      uses: actions/download-artifact@v3
      with:
        name: eval-results
    - name: Select best checkpoint
      run: |
        ruby lora_checkpoint_selector.rb \
          --eval-results eval_results.json \
          --scoring-weights '{"perplexity":-0.4,"rag_at_5":0.6}' \
          --output best_checkpoint.txt
    - name: Deploy to production
      run: |
        BEST=$(cat best_checkpoint.txt)
        echo "Deploying $BEST to production..."
        cp ./checkpoints/$BEST /opt/production/model.bin
        systemctl restart inference-service
```

---

## Complete CI Workflow (End-to-End)

### Full GitHub Actions YAML Example

```yaml
name: CPT/RAG Pipeline

on:
  schedule:
    - cron: '0 2 * * 1'  # Weekly on Monday 2am
  workflow_dispatch:     # Manual trigger

env:
  ROLLING_WINDOW_IDX: 5
  SEED: 1234
  REHEARSAL_RATIO: 0.3

jobs:
  ingest-and-split:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Ingest new emails
        run: ./scripts/ingest_emails.sh
      - name: Split chunks
        run: ruby splitter.rb --input-dir ./email_chunks --output assignments.json --append
      - name: Upload assignments
        uses: actions/upload-artifact@v3
        with:
          name: assignments
          path: |
            assignments.json
            length_metadata.json
            window_idx

  generate-rehearsal-manifest:
    needs: ingest-and-split
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: actions/download-artifact@v3
        with:
          name: assignments
      - name: Build manifest
        run: |
          ruby manifest_builder.rb \
            --assignments assignments.json \
            --length-metadata length_metadata.json \
            --window-idx ${{ env.ROLLING_WINDOW_IDX }} \
            --seed ${{ env.SEED }} \
            --caps '{"short":500,"medium":300,"long":200,"verylong":100}' \
            --output old_shards_train.json
      - name: Upload manifest
        uses: actions/upload-artifact@v3
        with:
          name: rehearsal-manifest
          path: old_shards_train.json

  train-lora:
    needs: generate-rehearsal-manifest
    runs-on: [self-hosted, gpu]  # GPU runner required
    steps:
      - uses: actions/checkout@v3
      - uses: actions/download-artifact@v3
      - name: Setup environment
        run: |
          pip install torch transformers peft accelerate
      - name: Train model
        run: |
          ruby retrain.rb \
            --assignments assignments.json \
            --rehearsal-manifest old_shards_train.json \
            --length-metadata length_metadata.json \
            --window-idx ${{ env.ROLLING_WINDOW_IDX }} \
            --epochs 3 \
            --seed ${{ env.SEED }} \
            --rehearsal-ratio ${{ env.REHEARSAL_RATIO }} \
            --batch-size 8 \
            --checkpoint-dir ./checkpoints/
      - name: Upload checkpoints
        uses: actions/upload-artifact@v3
        with:
          name: lora-checkpoints
          path: ./checkpoints/

  evaluate-checkpoints:
    needs: train-lora
    runs-on: [self-hosted, gpu]
    steps:
      - uses: actions/checkout@v3
      - uses: actions/download-artifact@v3
      - name: Run evaluation
        run: |
          ruby RAG_evaluator.rb \
            --checkpoint-dir ./checkpoints/ \
            --assignments assignments.json \
            --test-manifest test_chunks.json \
            --output eval_results.json
      - name: Upload results
        uses: actions/upload-artifact@v3
        with:
          name: eval-results
          path: eval_results.json

  select-and-deploy:
    needs: evaluate-checkpoints
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: actions/download-artifact@v3
      - name: Select best checkpoint
        run: |
          ruby lora_checkpoint_selector.rb \
            --eval-results eval_results.json \
            --scoring-weights '{"perplexity":-0.4,"rag_at_5":0.6}' \
            --output best_checkpoint.txt
      - name: Deploy to production
        run: |
          BEST=$(cat best_checkpoint.txt)
          echo "Deploying checkpoint: $BEST"
          ./scripts/deploy_to_production.sh ./checkpoints/$BEST

  notify:
    needs: select-and-deploy
    runs-on: ubuntu-latest
    if: always()
    steps:
      - name: Send notification
        run: |
          curl -X POST ${{ secrets.SLACK_WEBHOOK }} \
            -d '{"text":"CPT/RAG pipeline completed. Status: ${{ job.status }}"}'
```

---

## Key CI Workflow Principles

1. **Artifact Passing**: Each job produces artifacts consumed by downstream jobs (assignments → manifest → checkpoints → eval results)

2. **GPU Requirements**: `train-lora` and `evaluate-checkpoints` must run on GPU-enabled runners (self-hosted or cloud GPU instances)

3. **Determinism**: All RNG operations use `--seed` parameter for reproducible runs

4. **Idempotency**: 
   - `splitter.rb` appends only new IDs (safe to rerun)
   - `manifest_builder.rb` regenerates from scratch (deterministic)
   - Training can resume from checkpoints

5. **Failure Handling**:
   - Each job should upload artifacts even on partial completion
   - Use `if: always()` for cleanup/notification jobs
   - Store checksums for data integrity verification

6. **Monitoring**:
   - Log training metrics (loss, perplexity) to CI outputs
   - Archive eval_results.json for historical comparison
   - Alert on metric regressions

---

## Typical Execution Timeline

**Weekly Retrain Cycle**:
```
Monday 02:00 - ingest-and-split            (5 min)
Monday 02:05 - generate-rehearsal-manifest (2 min)
Monday 02:07 - train-lora                  (4-8 hours, GPU)
Monday 10:00 - evaluate-checkpoints        (1-2 hours, GPU)
Monday 12:00 - select-and-deploy           (5 min)
Monday 12:05 - notify                      (1 min)
```

**On-Demand Retrain** (manual dispatch):
- Trigger via GitHub Actions UI → `workflow_dispatch`
- Useful for emergency retrains or hyperparameter experiments

---

## Troubleshooting Common CI Issues

### Issue: GPU runner unavailable
**Solution**: Use queue timeout and retry logic:
```yaml
train-lora:
  timeout-minutes: 600  # 10hr max
  runs-on: [self-hosted, gpu]
  strategy:
    max-parallel: 1
```

### Issue: Artifact download fails
**Solution**: Add artifact existence checks:
```yaml
- name: Download artifacts
  uses: actions/download-artifact@v3
  with:
    name: assignments
  continue-on-error: false
- name: Verify artifacts
  run: |
    test -f assignments.json || exit 1
```

### Issue: Non-deterministic training results
**Solution**: Verify all RNG seeds are set:
- `--seed` in manifest_builder.rb
- `--seed` in sampler.rb  
- `--seed` in retrain.rb
- `torch.manual_seed()` in trainer.rb

### Issue: OOM during training
**Solution**: Reduce batch size or use gradient accumulation:
```yaml
- name: Train with gradient accumulation
  run: |
    ruby retrain.rb \
      --batch-size 4 \
      --gradient-accumulation-steps 2 \
      ...
```

---

## Summary

Your pipeline is a **production-grade ML system** with:
- **8 specialized scripts** handling split → manifest → sample → train → eval → select
- **Deterministic reproducibility** via SHA256-seeded RNG
- **CI orchestration** via GitHub Actions (or GitLab CI, Jenkins, etc.)
- **Stratified replay sampling** to prevent catastrophic forgetting
- **Rolling window** for incremental learning
- **Multi-objective checkpoint selection** for optimal deployment

Each script has a clear boundary: `splitter.rb` owns assignments, `manifest_builder.rb` generates rehearsals, `sampler.rb` schedules epochs, `retrain.rb` orchestrates training, `RAG_evaluator.rb` measures quality, and `lora_checkpoint_selector.rb` picks the winner. CI stitches them together into an automated, auditable, reproducible training pipeline.