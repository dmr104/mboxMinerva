# mboxMinerva

**Production-grade email archive LLM fine-tuning with immutable splits, PII safety, and RAG deployment**

[![CI Status](https://github.com/dmr104/mboxMinerva/actions/workflows/ci.yml/badge.svg)](https://github.com/dmr104/mboxMinerva/actions)
[![License](https://img.shields.io/badge/license-All%20Rights%20Reserved-red.svg)](LICENSE)

---
### My notes
PII = personally identififable information
RAG = retrieval augmentation generation
requirements.txt was generated by `pip freeze > requirements.txt`

base64 is not crypto; the actual protection is GPG/git-crypt encryption and secure storage of the secret variable, 
with masking merely preventing accidental exposure. Even with the exported key we never commit it to the repo; we base64 the git-crypt export key and store it only as a masked (hides the value in logs/UI) CI variable, then decode it in CI at job 
runtime to unlock the vault.  Adding GIT_CRYPT_KEY_BASE64 to CI secrets (base64 -w0 .git-crypt-key) means you store a 
base64-encoded git-crypt export key as a CI variable so the pipeline can unlock the repo; locally run:
git-crypt export-key .git-crypt-key && base64 -w0 .git-crypt-key;
and paste that output into GitLab settings => CI/CD => Variables as GIT_CRYPT_KEY_BASE64 (masked, protected),
then in .gitlab-ci.yml unlock with `echo "$GIT_CRYPT_KEY_BASE64" | base64 -d > .git-crypt-key && git-crypt unlock .git-crypt-key`

Or alternatively, use the GPG-user model: locally use
gpg --import private-key.asc; but in Gitlab CI we need a non-interactive import; in Gitlab CI we can use:
echo "GPG_PRIVATE_KEY" | gpg --batch --yes --pinentry-mode loopback --passphrase "GPG_PASSPHRASE" --import;
git-crypt add-gpg-user <COLLEAGUE_KEYID>;
git-crypt unlock;
Again, we never commit the private key to the repo 

Test: ruby -e "require_relative 'lib/vault_guard'; VaultGuard.ensure_unlocked!"

To Run vLLM’s OpenAI server and point your RAG at it - e.g. 
docker run --gpus all -p 8000:8000 -v /models:/models vllm/vllm-openai:latest --host 0.0.0.0 --port 8000 --model /models/your-llm --dtype float16 --max-model-len 4096 --gpu-memory-utilization 0.9 --tensor-parallel-size 1 --quantization awq 
(concurrency is automatic via continuous batching), set OPENAI_BASE_URL=http://vllm:8000/v1 (API key can be a placeholder), and in GitLab CI start this container on your GPU runner (as a service or compose) before your integration tests, then reuse the same compose for prod.


## Overview

mboxMinerva enables **continuous pre-training (CPT) on email archives** with:
- **Frozen train/val/test splits** that remain stable across incremental retraining
- **Thread-aware assignment** to prevent data leakage
- **PII scrubbing at ingestion** with deterministic pseudonymization
- **Stratified rehearsal sampling** to prevent catastrophic forgetting
- **Data Subject Request (DSR) compliance** with export/delete tooling
- **RAG baseline** (Postgres+pgvector) for email search

**Key Design Principles:**
1. **Reproducibility**: Same seed + data = identical splits (forever)
2. **Immutability**: Test set never contaminates training in future retrains
3. **Privacy-first**: PII scrubbed before splits, reversible for DSR compliance
4. **Incrementality**: New emails append deterministically without reshuffling

---

## Table of Contents

- [Prerequisites](#prerequisites)
- [Installation](#installation)
- [Quickstart](#quickstart)
- [Architecture](#architecture)
  - [Immutable Split Manifest](#immutable-split-manifest)
  - [PII Pipeline](#pii-pipeline)
  - [Training Workflow](#training-workflow)
- [Command Reference](#command-reference)
- [Data Subject Requests (DSR)](#data-subject-requests-dsr)
- [RAG Deployment](#rag-deployment)
- [Testing](#testing)
- [Security](#security)
- [Troubleshooting](#troubleshooting)
- [Contributing](#contributing)

---

## Prerequisites

- **Ruby** 3.2+ (`ruby --version`)
- **Python** 3.10+ (`python3 --version`)
- **Bundler** (`gem install bundler`)
- **PostgreSQL** 15+ with pgvector extension (for RAG)
- **Git LFS** (for large model checkpoints)

---

## Installation

### 1. Clone Repository
```bash
git clone https://github.com/dmr104/mboxMinerva.git
cd mboxMinerva
```

### 2. Install Ruby Dependencies
```bash
bundle install
```

**Gemfile** includes:
- `rspec` (testing framework)
- `json` (manifest parsing)
- Standard library gems (digest, optparse)

### 3. Install Python Dependencies
```bash
pip install -r requirements.txt
```

**requirements.txt** includes:
- `torch>=2.0.0` (model training)
- `transformers>=4.30.0` (LLM interface)
- `datasets>=2.12.0` (data loading)
- `psycopg2-binary` (PostgreSQL driver for RAG)

### 4. Set Up Vault (PII Storage)
```bash
mkdir -p vault
chmod 700 vault
# Encrypt vault with git-crypt (see Security section)
```

### 5. Configure Environment
```bash
cp .env.example .env
# Edit .env with database credentials, API keys
```

**.env** should contain:
```bash
POSTGRES_HOST=localhost
POSTGRES_DB=mboxminerva_rag
POSTGRES_USER=your_user
POSTGRES_PASSWORD=your_password
PII_VAULT_KEY=your_encryption_passphrase
```

---

## Quickstart

### End-to-End Example (Synthetic Data)

```bash
# Step 1: Parse raw mbox file
bin/mbox_pre-parser.rb examples/sample.mbox > data/raw.json

# Step 2: Scrub PII (deterministic pseudonymization)
ruby lib/pii_scrubber.rb \
  --seed 42 \
  --deterministic \
  --save-map vault/pseudonym_map.json \
  data/raw.json data/scrubbed.json

# Step 3: Create immutable splits (80/10/10 train/val/test)
bin/splitter.rb \
  -i data \
  -o splits \
  -m data/assignments.json \
  --incremental \
  -s 42 \
  --window-size 100 \
  --window-overlap 10

# Step 4: Materialize split files
bin/immutable_manifest.rb materialize \
  -m data/assignments.json \
  -o splits

# Step 5: Sample stratified rehearsal batch
bin/sampler.rb \
  data/old_train.json \
  splits/train.json \
  --replay-ratio 1:4 \
  --batch-size 16 \
  --min-per-bucket 2 \
  --seed 42 \
  --output data/mixed_train.json

# Step 6: Train LoRA adapter
bin/retrain.rb \
  --train data/mixed_train.json \
  --val splits/val.json \
  --base-model mistralai/Mistral-7B-v0.1 \
  --output-path models/checkpoint_v1 \
  --epochs 3 \
  --lr 5e-5 \
  --batch-size 16

# Step 7: Evaluate checkpoint
python3 scripts/eval_before_after.py \
  --base mistralai/Mistral-7B-v0.1 \
  --ft models/checkpoint_v1 \
  --test splits/test.json

# Step 8: Select best checkpoint (CI automation)
bin/lora_checkpoint_selector.rb \
  --checkpoints models/ \
  --test splits/test.json \
  --output best_checkpoint.txt

# Step 9: Merge LoRA adapter for deployment
bin/merge_lora.rb \
  --base mistralai/Mistral-7B-v0.1 \
  --adapter models/checkpoint_v1 \
  --output models/merged_v1.bin
```

---

## Architecture

### Immutable Split Manifest

**Core concept**: `assignments.json` is an **append-only map** where each ID (Message-Id, thread_id, or window_id) receives a **permanent split assignment**.

**Structure**:
```json
{
  "message123@example.com": {
    "split": "train",
    "thread_id": "thread_abc"
  },
  "thread_xyz_window_0": {
    "split": "val",
    "thread_id": "thread_xyz",
    "window_idx": 0
  }
}
```

**Properties**:
- **Immutable**: Existing entries never change
- **Append-only**: New IDs added on ingest; old IDs frozen
- **Deterministic**: Assignment via SHA256 hash-bucketing with 80/10/10 quotas
- **Thread-level**: All messages/windows in a thread share the same split

**Hash-Bucket Algorithm**:
```ruby
bucket = Digest::SHA256.hexdigest("#{thread_id}-#{seed}").to_i(16) % 100
split = case bucket
  when 0..79 then 'train'   # 80%
  when 80..89 then 'val'    # 10%
  else 'test'                # 10%
end
```

**Why this matters**:
- Same seed + same thread_id = same split (forever)
- No randomness = no accidental drift
- Scientific reproducibility: test set never leaks into training

---

### PII Pipeline

**1. Ingestion (PII Scrubbing)**

Tool: `lib/pii_scrubber.rb`

**What it does**:
- Pseudonymizes email addresses: `john@example.com` → `user_a3f8b2c1@example.com`
- Redacts phone numbers, SSNs, credit cards: `<PHONE>`, `<SSN>`, `<CARD>`
- Hashes IP addresses to RFC1918 private ranges

**Reversibility**:
- Pseudonym map stored in `vault/pseudonym_map.json` (encrypted)
- Original email addresses never enter training data

**Command**:
```bash
ruby lib/pii_scrubber.rb \
  --seed 42 \
  --deterministic \
  --save-map vault/pseudonym_map.json \
  input.json output.json
```

**2. Training Data Preparation**

Input: Scrubbed JSON → `splitter.rb` → train/val/test splits

**Guarantee**: Only scrubbed data enters `assignments.json`

**3. Model Training**

**Risk**: Model memorization of PII fragments

**Mitigation**:
- Use LoRA fine-tuning (low-rank adapters) instead of full fine-tuning
- Apply dropout (0.1) during training
- Monitor validation loss for overfitting

**Post-training audit**:
```bash
# Scan checkpoint for leaked PII patterns
grep -Eo '\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b' models/checkpoint.bin
```

---

### Training Workflow

**Pipeline**: `sampler.rb` → `retrain.rb` → `lora_checkpoint_selector.rb`

**1. Stratified Rehearsal Sampling**

Tool: `bin/sampler.rb`

**Purpose**: Prevent catastrophic forgetting during incremental training

**How it works**:
- Takes old training data (previous shards) and new training data
- Builds hierarchical buckets (sender → thread → samples)
- Guarantees minimum representation for rare senders/threads (default: 2 samples/bucket)
- Fills remaining quota proportionally (1:4 old:new ratio)

**Example**:
```bash
bin/sampler.rb \
  data/old_train.json \
  data/new_train.json \
  --replay-ratio 1:4 \
  --batch-size 16 \
  --min-per-bucket 2 \
  --seed 42 \
  --output data/mixed_train.json
```

**2. LoRA Fine-Tuning**

Tool: `bin/retrain.rb`

**Key parameters**:
- `--base-model`: Pretrained LLM (e.g., Mistral-7B)
- `--train`: Training manifest (mixed_train.json from sampler)
- `--val`: Validation split (val.json)
- `--epochs`: Training epochs (default: 3)
- `--lr`: Learning rate (default: 5e-5)
- `--save-train-shard`: Output directory for sharded checkpoints

**3. Checkpoint Selection**

Tool: `bin/lora_checkpoint_selector.rb`

**Scoring**: Weighted combination of:
- Perplexity on test split (lower is better)
- RAG@K accuracy (higher is better)

**Output**: `best_checkpoint.txt` with path to winning checkpoint

---

## Command Reference

### Core Pipeline Tools

#### `bin/mbox_pre-parser.rb`
Parse raw mbox into JSON with Message-Id, thread_id, subject, body.

```bash
bin/mbox_pre-parser.rb archive.mbox > output.json
```

#### `bin/splitter.rb`
Assign/append IDs to immutable manifest and create splits.

```bash
bin/splitter.rb \
  -i <input_dir> \
  -o <output_dir> \
  -m <manifest_file> \
  --incremental \
  -s <seed> \
  --window-size <size> \
  --window-overlap <overlap>
```

**Flags**:
- `-i`: Input directory with intermediate.json
- `-o`: Output directory for split JSONLs
- `-m`: Manifest file (reused forever)
- `--incremental`: Only process new IDs not in manifest
- `-s`: Hash seed (never change after first run!)
- `--window-size`: Chunking for mega-threads (default: 100)
- `--window-overlap`: Overlap between windows (default: 10)

#### `bin/immutable_manifest.rb`
Utility for manifest operations.

```bash
# Add a single ID manually
bin/immutable_manifest.rb assign \
  -m assignments.json \
  -i msg@example.com \
  -t thread_abc \
  -s 42

# Materialize split lists from manifest
bin/immutable_manifest.rb materialize \
  -m assignments.json \
  -o splits

# Inspect manifest stats
bin/immutable_manifest.rb stats -m assignments.json
```

#### `bin/sampler.rb`
Generate stratified rehearsal batch.

```bash
bin/sampler.rb <old_train> <new_train> \
  --replay-ratio 1:4 \
  --batch-size 16 \
  --min-per-bucket 2 \
  --seed 42 \
  --output mixed_train.json
```

#### `bin/retrain.rb`
Train LoRA adapter with rehearsal sampling.

```bash
bin/retrain.rb \
  --train mixed_train.json \
  --val val.json \
  --base-model <model_name> \
  --output-path <checkpoint_dir> \
  --epochs 3 \
  --lr 5e-5 \
  --batch-size 16
```

#### `bin/lora_checkpoint_selector.rb`
Select best checkpoint based on eval metrics.

```bash
bin/lora_checkpoint_selector.rb \
  --checkpoints models/ \
  --test splits/test.json \
  --output best_checkpoint.txt
```

#### `bin/merge_lora.rb`
Merge LoRA adapter into base model for deployment.

```bash
bin/merge_lora.rb \
  --base <base_model> \
  --adapter <adapter_path> \
  --output merged_model.bin
```

---

### Data Subject Request (DSR) Tools

#### `bin/dsr_export`
Export all data for a given subject (email or pseudonym).

```bash
bin/dsr_export \
  --subject user@example.com \
  --vault vault/pseudonym_map.json \
  --splits data/assignments.json \
  --threads \
  --output exports/user_data.jsonl
```

**Flags**:
- `--subject`: Email address or pseudonym to export
- `--vault`: Path to pseudonym map (default: vault/pseudonym_map.json)
- `--splits`: Path to manifest (default: data/assignments.json)
- `--threads`: Include full threads (not just subject's messages)
- `--output`: Export file path

**Output format** (JSONL):
```json
{"type": "email", "message_id": "...", "split": "train", ...}
{"type": "email", "message_id": "...", "split": "val", ...}
```

Includes summary:
```json
{"summary": {"total_records": 42, "splits": {"train": 35, "val": 5, "test": 2}}}
```

#### `bin/dsr_delete`
Mark records for deletion via append-only tombstones.

```bash
bin/dsr_delete \
  --subject user@example.com \
  --vault vault/pseudonym_map.json \
  --splits data/assignments.json \
  --tombstones vault/dsr_tombstones.jsonl \
  --threads \
  --dry-run
```

**Flags**:
- `--subject`: Email address or pseudonym to delete
- `--vault`: Path to pseudonym map
- `--splits`: Path to manifest
- `--tombstones`: Tombstone file (default: vault/dsr_tombstones.jsonl)
- `--threads`: Delete entire threads (not just subject's messages)
- `--dry-run`: Preview deletion without writing tombstones

**Tombstone format**:
```json
{"chunk_id": "msg123@example.com", "timestamp": "2025-11-09T19:00:00Z", "reason": "DSR deletion"}
```

**Important**: Tombstones are **append-only**. Never mutate `assignments.json`.

#### Dataloader Integration

`lib/dataloader.rb` automatically filters tombstoned records:

```ruby
require_relative 'lib/dataloader'

loader = DataLoader.new(
  'splits/train.json',
  respect_tombstones: true,  # Default
  tombstones_path: 'vault/dsr_tombstones.jsonl'
)
```

**Behavior**:
- Loads tombstones at initialization
- Filters deleted chunk_ids from training schedule
- Preserves determinism (same manifest + tombstones = same output)

---

## RAG Deployment

**Status**: Baseline implementation in progress

**Architecture**:
1. **Embedding Generation**: Sentence-transformers or OpenAI Ada-002
2. **Vector Storage**: Postgres with pgvector extension
3. **Query Pipeline**: Retrieve top-K chunks, pass to LLM for synthesis

### Setup Postgres+pgvector

```bash
# Install pgvector extension
psql -d mboxminerva_rag -c "CREATE EXTENSION IF NOT EXISTS vector;"

# Create embeddings table
psql -d mboxminerva_rag -f scripts/init_rag_schema.sql
```

**Schema** (simplified):
```sql
CREATE TABLE embeddings (
  chunk_id TEXT PRIMARY KEY,
  embedding vector(768),  -- Dimension depends on model
  split TEXT,             -- train/val/test (for eval only)
  metadata JSONB          -- thread_id, sender, timestamp
);
```

### Index Embeddings

```bash
ruby lib/rag_index_builder.rb \
  --input splits/train.json \
  --output embeddings.db \
  --model sentence-transformers/all-MiniLM-L6-v2
```

### Evaluate RAG Baseline

```bash
bin/RAG_evaluator.rb \
  --db postgres://localhost/mboxminerva_rag \
  --test-queries splits/test_queries.json \
  --k 5 \
  --output rag_metrics.json
```

**Metrics**:
- Recall@K: Fraction of test queries where correct answer in top K
- MRR (Mean Reciprocal Rank): Average 1/rank of first correct answer
- Precision@K: Fraction of retrieved chunks that are relevant

---

## Testing

### Run All Tests
```bash
bundle exec rspec spec/
```

### Split Integrity Tests

File: `spec/split_integrity_spec.rb`

**Checks**:
- Manifest immutability (no overwrites on incremental runs)
- Thread-level assignment consistency (all messages in thread share split)
- 80/10/10 distribution enforcement (± 2% tolerance)
- Determinism verification (same seed + data = identical manifest)

**Example**:
```ruby
RSpec.describe 'Split Integrity' do
  it 'enforces immutability on incremental append' do
    manifest_v1 = run_splitter(seed: 42, data: 'emails_batch1.json')
    manifest_v2 = run_splitter(seed: 42, data: 'emails_batch1.json + emails_batch2.json')
    
    # Existing entries unchanged
    expect(manifest_v2.slice(*manifest_v1.keys)).to eq(manifest_v1)
  end
end
```

### CI/CD Integration

**GitHub Actions** (`.github/workflows/ci.yml`):
```yaml
name: CI
on: [push, pull_request]
jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: ruby/setup-ruby@v1
        with:
          ruby-version: 3.2
          bundler-cache: true
      - run: bundle exec rspec spec/
      
  secret-scan:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: trufflesecurity/trufflehog@main
        with:
          path: ./
          base: main
```

---

## Security

### Vault Encryption

**Tool**: `git-crypt` or `sops`

**Setup** (git-crypt):
```bash
# Initialize git-crypt in repo
git-crypt init

# Add encryption rules
cat >> .gitattributes <<EOF
vault/** filter=git-crypt diff=git-crypt
.env filter=git-crypt diff=git-crypt
EOF

# Add collaborators
git-crypt add-gpg-user user@example.com
```

**Verification**:
```bash
# Locked state (encrypted on disk)
git-crypt lock

# Check vault is encrypted
file vault/pseudonym_map.json  # Output: data
```

### Secrets Management

**Never commit**:
- `.env` files (database credentials, API keys)
- `vault/pseudonym_map.json` (PII mapping)
- Raw mbox files

**Use**:
- GitHub Secrets for CI/CD workflows
- Environment variables for local development
- Encrypted vault with passphrase rotation every 90 days

### Access Control

**Vault permissions**:
```bash
chmod 700 vault/
chmod 600 vault/pseudonym_map.json
```

**Database**:
- TLS for connections (enforce with `PGSSLMODE=require`)
- Row-level security if multi-tenant
- Rotate credentials quarterly

### Audit Logging

**What to log**:
- DSR export/delete requests (who, when, what)
- Manifest edits (timestamp, diff)
- Model checkpoint access (production deployments)

**Where to log**:
- `logs/audit.jsonl` (append-only, tamper-evident)
- Centralized SIEM (Splunk, ELK) for production

---

## Troubleshooting

### Common Errors

**1. Missing vault file**
```
Error: vault/pseudonym_map.json not found
```

**Solution**:
```bash
mkdir -p vault
# Re-run PII scrubber with --save-map
```

**2. Seed mismatch**
```
Warning: Seed changed from 42 to 99. Splits will be recomputed.
```

**Solution**: Never change the seed after first run. If you must, delete `assignments.json` and start fresh.

**3. Thread split contamination**
```
RSpec failure: Thread thread_xyz has messages in both train and test
```

**Solution**: This indicates a bug in `splitter.rb`. File an issue with:
- Manifest file
- Input data
- Ruby version

**4. Tombstone not filtering**
```
Warning: Dataloader loaded 42 records, expected 40 (2 tombstoned)
```

**Solution**: Check tombstone format is valid JSONL with `chunk_id` field.

### Debug Mode

Enable verbose logging:
```bash
export MBOX_MINERVA_DEBUG=1
bin/splitter.rb ...  # Logs full trace
```

---

## Contributing

### Code Style

**Ruby**:
- No shebangs in `lib/` files (only in `bin/`)
- Use `require_relative` for local modules
- Follow RuboCop style guide

**Python**:
- Black formatting (`black scripts/`)
- Type hints for function signatures
- Docstrings for public functions

### Pull Request Checklist

- [ ] Tests pass (`bundle exec rspec spec/`)
- [ ] Code formatted (`rubocop --auto-correct`)
- [ ] Docs updated (`README.md`, `docs/`)
- [ ] Changelog entry (`CHANGELOG.md`)
- [ ] No secrets committed (`.env`, `vault/`)

### Reporting Issues

Include:
- Ruby/Python version
- OS (macOS, Linux, Windows/WSL)
- Full error trace
- Minimal reproducible example

---

## Design Decisions FAQ

### Why hash-bucketing instead of random sampling?

**Determinism**: Random splits are non-reproducible without serializing the entire RNG state. Hash-bucketing guarantees the same thread always lands in the same bucket given the same seed.

### Why append-only manifest?

**Frozen reproducibility**: Never mutate existing assignments. Scientific experiments require fixed test sets. Adding new data shouldn't change how we evaluate old performance.

### Why thread-level assignment?

**Leakage prevention**: If message A and message B are in the same conversation, training on A and testing on B violates independence. Thread-level assignment prevents this.

### Why do windows inherit thread splits?

**Same reason**: Windows are slices of one conversation, albeit overlapping ones. Training on `thread_xyz_window_0` and testing on `thread_xyz_window_1` is leakage.

### Why two-layer design (manifest + materialize)?

**Efficiency**: Manifest is a compact map; materialization generates full splits on demand. Supports multiple export formats (JSONL, CSV) from one source of truth.

---

## Roadmap

**Current Status: v0.9 (Pre-release)**

### v1.0 (Q1 2026)
- [x] Immutable split architecture
- [x] PII scrubbing at ingestion
- [x] DSR export/delete tooling
- [x] Split integrity tests
- [ ] CI/CD automation (GitHub Actions)
- [ ] RAG baseline (Postgres+pgvector)
- [ ] Eval harness (scripts/eval_before_after.py)

### v1.1 (Q2 2026)
- [ ] Differential privacy (DP-SGD)
- [ ] Model versioning and tagging
- [ ] Performance optimization (caching, indexing)
- [ ] Multi-mbox support (federated learning)

### v2.0 (Q3 2026)
- [ ] Active learning loop
- [ ] Web UI for DSR management
- [ ] Cloud deployment templates (AWS, GCP, Azure)

---

## License

© 2025 David Roderick. All Rights Reserved.

No warranty provided. See `LICENSE` for full terms.

---

## Citation

If you use mboxMinerva in your research, please cite:

```bibtex
@software{mboxminerva2025,
  author = {Roderick, David},
  title = {mboxMinerva: Privacy-Safe Email LLM Training},
  year = {2025},
  url = {https://github.com/dmr104/mboxMinerva}
}
```

---

## Acknowledgments

- Immutable manifest design inspired by [DVC](https://dvc.org/) and [Pachyderm](https://www.pachyderm.com/)
- Stratified rehearsal sampling adapted from [GEM benchmark](https://gem-benchmark.com/)
- PII scrubbing patterns from [Microsoft Presidio](https://microsoft.github.io/presidio/)

---

## Contact

- **GitHub Issues**: [dmr104/mboxMinerva/issues](https://github.com/dmr104/mboxMinerva/issues)
- **Email**: [contact form on profile]
- **Docs**: See `docs/` directory for detailed guides

---

**The Golden Rule**: Treat `assignments.json` as **permanent marker**. Once written, never erase. Only append.

**Result**: Scientifically reproducible, incrementally trainable, privacy-safe email AI.