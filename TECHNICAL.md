## What is a cohort?
A cohort is just a timestamped batch of rows within the manifest file "assignments.json", e.g. "2025-01" which tags all emails ingested in that period so we can freeze, and pin, and talk about the data that existed as of that particular cohort_id as to be considered within each split, for retrains and audits.

A cohort_id (e.g. 2025-01) is the stable tag for a cohort, and a cohort is simply a group of emails that arrived during this bucketed time-interval, say, 1 month. Because the file "bin/splitter.rb" is normally run weekly with `--materialize train` and a fixed cohort pin, it filters from the append-only manifest and writes exclusively to a newly versioned train.jsonl monolithic file. 

A fixed cohort pin is the explicit cutoff tag (e.g. cohort_id=2025-01) that val.jsonl and test.jsonl are locked to, so that if I do a planned rollover yearly, and have no DSRs within this time, my explicit cutoff will change only once per year at the planned bump. This doesn't prevent us doing an ad-hoc bump if drift gets too bad.  Drift is a distribution mismatch between what the model has as data we have already fitted, and what present traffic contains.

At what stage does the cohort_id get written into the immutable manifest rows?  Answer. At ingest time:  when "bin/mbox_pre-parser.rb" appends new rows, it stamps cohort_id (usually YYYY-MM as 'received_at:' which is from within the email data, or from the latest configured batch cutoff as specified as a command line argument to "mbox_pre-parser.rb" as the --cohort argument).  

## What is a split?
A split is the role tag on each manifest row (train, val, or test) within "assignments.json" which controls which "split file" it materializes into (train.jsonl, val.jsonl, or test.jsonl), and and how it updates (train can be re-cut anytime; val and test stay pinned and only change on DSR subtracts or a deliberate pin bump).  

## What is a pin bump?
A pin bump is the deliberate advancing done to the cohort_id cutoff for val and test (e.g. 2025-01 goes to 2025-07), followed by rematerialization of those splits to include the newer cohorts. 

## What is a rollover?
A planned rollover involves the flipping of a symlink.  This symlink may point to the actual model checkpoint (LoRA adapter) directory, which may reside, for example, at `current/releases/2025-01-15-clean` so that flipping the symlink would atomically switch from serving the old adapter to the newly trained DSR-clean one without changing any runtime configurations.


## What is a materialization?
Materialization is the process of extracting previously split data from the immutable manifest (the file "assignments.json") and writing the results to the files "train.jsonl", "val.jsonl", and "test.jsonl". 

## What is a retrain?
The difference between a retrain and rematerialization is that during a retrain we are actually retraining LoRA adapters to fit on top of an existing large language model, while a rematerialization is when the files (train/val/test.jsonl) which the latest model reads, are deterministically rebuilt from our immutable manifest "assignments.json".  Upon rematerialization, the data which is tombstoned in the "assignments.json" simply does not get written into any of the new train/val/test.jsonl. We retrain the model from its base checkpoint by creating a new LoRA adaptor and refitting it: it is like painting a new canvas (retraining), as opposed to merely touching up the old one (remateralization).    

What would happen if I bump the pin, and then receive a DSR deletion request for data which exists within a previous cohort_id?  Does a `--materialize all` option to splitter.rb wipe its data out within these files?  Answer.  Yes.

So, if I retrain the model using this newer train/val (with those tombstones), in practice the trained model *replaces* the previous adapter which was upon the base model.  You don't layer adapters in order to forget things.  Instead you swap in a freshly trained one that never saw the deleted rows in the first place.  As we retrain when specific key performance indicators are breached, OR upon a fixed cadence, say "max staleness" as a time period between every 6 to 12 months, thus upon a receipt of a DSR deletion request, we retrain upon whichever comes first; and hence we may fulfill legal or contractual obligations to have done so within the service level agreement which may have stipulated a clause such like "the model is always up to date with data such that the data it is trained upon is never older than 6 months prior to the date of the present moment, and hence DSRs are always updated to this model (i.e. deleted from it) periodically every six months, or sooner".

When you train with incoming newer data (emails), you can re-materialize exclusively to train.jsonl to absorb new emails from existing cohorts, while val/test stay frozen so that benchmarks don't move, i.e. assuming that we have no DSR requests within this time interval, you can keep re-cutting under the old pin; but val/test move only when you bump the pin.  

## What about Data Subject Requests (DSRs)?
When a DSR request comes in, we tombstone the data in the immutable manifest file ("assignments.json") and later trigger a clean rematerialization (without bumping the pin).  Pin bumps are an explicit operational decision (e.g. a "roll forward" event), not something that happens automatically as part of a deletion request.  The file as "bin/splitter.rb" is the CLI (command line interface) we invoke to materialize or rematerialize splits from the immutable manifest (e.g. `splitter.rb --pin 2025-01 --materialize train` will trigger a clean rematerialization of "train.jsonl", including all cohorts prior to that particular date, excluding tombstoned rows, but won't touch val/test unless we do `splitter.rb --pin 2025-01 --materialize all` which will rematerialize train/val/test using only cohorts with cohort_id <= 2025-01, and which won't include newer cohorts, and won't change the pre-existing composition of what already got put into val and test--beyond DSR effects--but may update)val and test up to and including emails received at 2025-01-31 23:59.

## How does my split data grow?
If I do a `splitter.rb --pin 2025-01 --materialize all` and a year later I do a `splitter.rb --pin 2026-01 --materialize all`, then, for example, the possibility exists that a thread from 2025-04 may enter test.jsonl or val.jsonl, as we are specifically expanding the "Time Horizon" to include everything up to that new date, whereby the April 2025 thread transitions from being an "ignored future data" (in the 2025 context) to being "eligible historical data" (in the 2026 context), and will enter the lottery as to where it lands based upon its hash and your split ratio.

## How does splitter.rb start out?
For the first full cut from "bin/mbox_pre-parser.rb" we run something like `bin/splitter.rb --manifest data/manifest.jsonl --pin 2025-01 --materialize all --out-dir data/splits` to deterministically assign email threads and emit "train.jsonl", "val.jsonl", and "test.jsonl" for training under that initial pin.  The --pin argument is something which is set when the script in invoked, i.e. if all my emails thus far are earlier than 2025-01, then 2025-01 will do it, and we can keep rematerializing "train.jsonl" at the start of each month without bumping the pin, and bump the pin every, say, 6 months, or 12 months, (or sooner if **drift** or the **exclusion-backlog** shows that our eval is getting stale) in order to let newer cohorts into val/test and refresh our benchmarks in a controlled step-change rather than a constant creep.

## What is drift?
Drift is the gap that opens when the distribution or meaning of data coming in shifts away from what the model was trained/evaluated upon. Think of data drift as something that happens when the data being input changes; and concept drift is when the output of the model changes correctly.  Label drift is when the class mix changes: that is, the proportion of each type of email in our data changes (for instance if on a professional mailing list a lot of emails arrive talking about fluffy dogs), and when that mix changes the mix shifts, the model's expectations drift, so we watch it and rebalance training and thresholds to keep the specific metrics we track to be within acceptable thresholds, and this in turn leads to an improved customer experience in accordance with the service level agreements.  In short, drift is a distribution mismatch between what the model has as data we have already fitted, and thus measure against, and what real traffic (and thus what the manifest) contains.

## What is exclusion-backlog?
Exclusion-backlog is simply the growing pile of new emails the model has to ignore under the current pin (newer cohorts and quarantined threads).  We measure it as a count and as a percentage of recently receive email data that is out-of-scope for train/val/test, and once that count or percentage passes a threshold this is our cue to bump the pin or refresh the model.

## What about automatic notifications and included advice?
We bake in email and Slack/webhooks so that when exclusion-backlog or drift indicators cross a configurable threshold the admin gets a message that (a) shows the current stats, (b) states which key performance area this indicator pertains to, and (c) recommends a definite action, such as "time to bump the pin", or "time to schedule a retrain on cohorts less than or equal to a specific PIN, or "tighten contamination thresholds for these cohorts".  To wire it into your repo, edit `config/alerts.yml` with your SMTP/Slack URLs, and schedule via cron (`0 9 * * 1`) or GitLab pipeline schedules, i.e. when exclusion-backlog hits 15% it'll tell you "bump the pin to 2025-04", when contamination crosses 1% it recommends tightening thresholds, and when tombstones pile up past 100 it nudges you toward a retrain.

## Why does mbox_pre-parser.rb output shard files?
Notice that "splitter.rb" has an input argument `-i EMAILS_DIR` but not a specific input file (which is the output file from "mbox_pre-parser.rb").  This is intentional, as instead of a single file, "splitter.rb" walks over all the sharded pre-parsed files in that directory (the outputs from "mbox_pre-parser.rb") so that it can deterministically assign whole threads to splits across the full range of data in one pass.  Shards are non-overlapping.  "mbox_pre-parser.rb" walks messages in order and assigns each one to exactly one part-XXXXX.jsonl file, so that together the shards are just a clean partition of the body of emails rather than copied of each other.  Note that for simplicity and downstream tooling, the outputs from "splitter.rb" are materialized as single flat files like "train.jsonl" / "val.jsonl" / "test.jsonl".

Raw mboxes are often one huge file per list or month.  The pre-parser converts the physical MBOX into logical JSONL Rows.  Instead, if it would output one file per execution, and was executed upon one MBOX, it might produce only one JSONL shard.  A **Logical Row** is the *atom* (one single email or thread entry), while a **Shard** is the *bucket* (the actual .jsonl file holding thousands of those atoms).  The pre-parser outputs shards so that the downstream tools can process data in parallel chunks instead of choking upon one massive 50GB file.

In our code base there is no ruby file that chops train.jsonl into shards -- "splitter.rb" merely produces one flat "train.jsonl" file, and the actual "sharding" happens later inside the training stack's data loader (e.g. the finetune script, / vLLM or PyTorch+DeepSpeed job that reads "train.jsonl" and automatically splits it across workers).

## What is a sliding window?
The `--window-size` option to "splitter.rb" is an argument which tells "splitter.rb" how big a **sliding window** of "recent history" of messages and cohorts is to be considered relative to the pin when balancing train/val/test such that "splitter.rb" can keep a specific ratio healthy on *recent* newly arriving email data without touching the older data frozen in the immutable manifest "assignments.json", i.e. only data inside the moving window is eligible for the split.  The `--window-size` option is also known as defining your **Rolling Retention Policy** (or "Lookback Horizon")--it tells the splitter to ignore data older than `N` days/months relative to the Pin, ensuring that your model trains only on relevant, recent patterns and isn't polluted by ancient, drifted history: drifted because the "ground truth" changes as the world evolves; vocabulary shifts (new slang evolves, old terms become deprecated), spammers use newer tactics to evade filters, and crucially, the structure of business data within an organisational structure might change, (i.e. a "purchase order" from 2018 might look completely different than one from 2025), meaning that patterns from very old data might mislead the model about today's reality.  "splitter.rb" never breaks threads, and it just limits the balancing logic to the last N units instead of the entire history.  We are saying that if we retrain with a --window-size of 5 years, then data from 15 years ago and 14 years ago will be excluded from within this particular training session as the data from 14 years ago falls outside this particular sliding window and will *not* be used in the training run.

## What are thread segments?
The `mbox_pre-parser.rb` can and often does chop a long email thread into multiple segments to fit context limits, which are ceilings upon the amount of information (measured in "tokens", a token being roughly 0.75 words) a llm (large language model) can hold in its "short-term memory" instantanteously (e.g. 4,096 or 8,192 tokens).  mbox files are just dumb records as flat lists of emails stored in the order of their arrival which can often be an interleaved order of arrival. An mbox has no inherent concept of "threads" or "token windows".  If a thread exceeds this limit, then the pre-parser will chop it into smaller "segments" to feed in to the llm, otherwise the llm effectively crashes or truncates the overflow.  This is also called "chunking", or "windowing".

## Don't suddenly change your splitter seed or configured ratio! 
"bin/splitter.rb" groups by thread_id, and always hashes with a deterministic seed to assign train/val/test (80/10/10) to the immutable manifest, writing immutably to assignments.json.  To say this again, splitter.rb assigns per-thread splits using a deterministic hash (seeded) to hit a fixed ratio so that the inputs always map to the same split in the immutable manifest unless you change the seed or configured ratio (which you should not do midstream because this would invalidate previous assignments; and IF YOU DO do this then you ***MUST*** recreate the **whole** manifest again and then materialize it!--effectively wiping the slate clean). 

## Ommiting --window-size
If the pre-parser splits a long thread into segments/chunks and then the `--window-size` option to `splitter.rb` is omitted, then as the pre-parser chops the thread then that single thread becomes **multiple distinct entries** (one row per chunk) in the manifest file "assignments.jsonl"; however because they all carry the same `thread_id`, the splitter treats those multiple rows singly logically, forcing them all into the same bucket so you don't fracture the conversation between train, val and test.

## Using --window-size
If the pre-parser splits a long thread into segments/chunks and then the `--window-size` option to `splitter.rb` is used, then as the pre-parser chops the thread then that single thread becomes **multiple distinct entries** (one row per chunk) in the manifest file "assignments.jsonl" regardless of the `--window-size` option because this option just tells the splitter to check the thread's age and either keep *all* those chunk-entries or discard *all* of them depending upon the criterion as comparing the **Thread's latest timestamp** (it's "freshness") against the cutoff window (the "pin date" minus the "window size"); if the conversation was active *at all* within that cutoff window, then the **entire** family of chunks is kept to preserve narrative context, while threads that "went cold" before that date are discarded entirely. 

## What does splitter actually do again?
How does the splitter know whether there exist any particular chunks which exist for that thread which are outside of the cutoff window being prior to it?  Answer.  It relies upon the pre-parser being smart: when the pre-parser originally processes the thread, it identifies the **single latest timestamp** of the entire conversation (the "high-water" mark) and stamps that value onto **every single chunk** it outputs.  So the splitter doesn't need to "search" for other chunks because it simply reads a particular `thread_last_active_at` date which is on the current row to issue a pass/fail verdict upon the whole family.

So a thread was active in 2024-12 and also in 2025-02 and if my pin=2026-01 and my --window-size=12months, then the splitter doesn't "know" about the chunks from 2024-12.  These already are in the manifest presumably as a row that the pre-parser produced; and splitter.rb just reads rows from the manifest, either keeping or dropping them and assigning to either train, val, or test. 

So in all cases, there is not any context leakage across train/val/test."

## What are RAG Shards?
In the RAG (retrieval) world, a **Shard** is an horizontal partition of your **Vector Index**" (the database holding your embeddings)--basically, instead of one giant searchable map that chokes a single server, you slice the map into pieces (shards) distributed across multiple node so you can search them in parallel (where "nodes" means the individual machines or logical database instances in the cluster);  this is distinct from "manifest shards" which are just the split-up JSONL files that each hold many logical rows so you don't end up with one giant unmanageable manifest file.  These "manifest shards" are storage/processing chunks of the manifest, not semantic units like threads or cohorts.

## How do nodes work?
Each node hold one or more RAG Shards of the index and runs it own little search engine, and a co-ordinator fans your query out to those nodes and then merges their results back together.

## How do we validate that the newer LoRA adapter does a better job than the last one?
We validate by running both adapters on the same frozen test set, checking that the new one wins on our key performance indicators (accuracy, helpfulness, safety, inbox quality), and doesn't regress on guardrails. These guardrails are the **runtime safety and compliance filters** (checking for PII, hallucinations, or toxicity) that intervene during the process of inference, and the frozen test set (test.jsonl) serves as the **unseen calibration set** used to measure the **False Positive Rate** of these guardrails--thus verifying that these safety rules aren't so aggressive that they accidentally block valid *future* use-cases from training the language model during a rollover pin bump.  The model + guardrails should allow and handle correctly future use-cases rather than blocking them as unsafe or as being outside of the distribution.

## What about my key to the crypt which stores the encrypted emails addresses?
In practice, the crypt key lives in a KMS (hardware-backed key management service) or HSM (hardware security module), rather than in code or in configuration files.  A KMS is something like Amazon Web Service KMS, or Google Cloud Platform KMS; and a HSM is a tamper-resistent hardware box that protects those keys so they can be used (e.g. to decrypt the crypt without anyone ever seeing or exporting the raw secret).  For self-hosted runners the *real* crypt key lives in an external secret store, and GitLab injects only a short-lived masked secret into the runner at job runtime such that the key is never within the repo, and it won't be baked into Docker images, and is scoped to specific projects and environments, so that it only exists within RAM on that runner whilst the particular CI job is decrypting the crypt.

HashiCorps Vault (or Openbao) is a self-hosted secrets manager that acts as a locked safe for passwords, API keys, and encyption keys, giving you a central place to store them encrypted, and to fetch short-lived credentials at runtime instead of hardcoding them in configs or in GitLab.

On-premises KMS/HSM means that instead of storing your encryption keys in a public cloud, they stay logically and physically under your control via your organisation running its own key-management system and hardware security modules within *your* own data centre which will be managed still by a central locked-down service.

A hardened OS/Kubernetes secrets backend is basically "Vault-lite" in the sense that you store secrets in the Kubernetes store which runs atop of the underlying operating system which is running on your servers (typically a hardened Linux distro like Ubuntu, Debian, or RHEL).  You make sure that the secrets which are stored in the Kubernetes store are encrypted at rest, and you lock the read-access to a tiny set of service accounts, injecting them into jobs only at runtime via the environment variables (temporary key=value settings which are visible only to that running process) or ephemeral volumes  (temporary filesystem mounts that exist only while the container/pod is alive). Every user/service gets the *minimum* permissions they need and nothing more, so only a few well-defined identities are ever allowed to read or use the encyption key.  You lock down each server which is within your cluster by disabling unused services. You lock down ports (by using a firewall on each host), and you enforce strong authentification like SSH keys, or 2-factor authentification, or SSO (Single Sign-On) with identity providers, such that an attacker will need more than one stolen credential to break in.  With SSO, you log in once with a central identity provider (which is an infrastructure--which is an identity provider service like Okta, Azure AD, or Google Workspace) that checks your login (password, multi-factor authentification, etc) and validates your account, and then issues short-lived tokens trusted tokens that the other apps accept as proof of who you are.

In any case, whichever key management system you choose to use, the idea is that the crypt key will never live within images, repos, or on long-lived disks and thus would be very hard to exfiltrate even if a node becomes compromised. 