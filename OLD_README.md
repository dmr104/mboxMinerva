# Aims of this repo

This repo is an attempt to fine-tune an AI model based upon the contents of an mbox, and then implement RAG upon this mbox based upon utilizing this fine-tuned llm.

# The following was generated by AI

# Email CPT/RAG Pipeline: Immutable Split Architecture Tutorial

## Overview

This pipeline enables **production-grade continuous pre-training (CPT) on email archives** with **frozen train/val/test splits** that remain stable across incremental retraining cycles. Key design goals:

1. **Reproducibility**: Once an email is assigned to train/val/test, that assignment never changes
2. **Incrementality**: New emails can be ingested without reshuffling existing assignments
3. **Thread awareness**: Emails in the same conversation thread stay in the same split
4. **Scalability**: Handle 45k+ emails with rolling-window chunking for mega-threads

---

## Core Concepts

### ID Taxonomy

The system works with three types of stable identifiers:

- **Message-Id**: Unique email header (e.g., `<abc123@example.com>`)
- **thread_id**: Computed from conversation grouping (e.g., `thread_xyz`)
- **window_id**: For chunked mega-threads (e.g., `thread_xyz_window_0`)

Each ID receives a **permanent train/val/test assignment** recorded in the immutable split manifest.

### Immutable Split Manifest (`assignments.json`)

A single **append-only map** structure:

```json
{
  "message123@example.com": {
    "split": "train",
    "thread_id": "thread_abc"
  },
  "thread_xyz_window_0": {
    "split": "val",
    "thread_id": "thread_xyz",
    "window_idx": 0
  }
}
```

**Key properties**:
- **Immutable**: Existing entries never change
- **Append-only**: New IDs added on ingest; old IDs frozen
- **Deterministic**: Assignment via SHA256 hash-bucketing with quotas (80/10/10 train/val/test)
- **Thread-level**: All messages/windows in a thread share the same split

Think of it as **Hogwarts sorting with permanent marker** - once sorted, always sorted.

---

## Architecture Components

sampler.rb runs **before** training to interleave stratified rehearsal results with new data (1:4 ration),
RAG_evaluator.rb triggers **after** training via continuous integration to compute perplexity + RAG@K for 
each checkpoint, and lora_checkpoint_selector.rb picks the winner using weighted scoring 
(perplexity + RAG accuracy) and writes best_checkpoint.txt for production deployment.

### 1. Hash-Bucket Deterministic Assignment

Instead of random shuffling, we use **deterministic hash bucketing**:

```ruby
bucket = Digest::SHA256.hexdigest("#{thread_id}-#{seed}").to_i(16) % 100
split = case bucket
  when 0..79 then 'train'   # 80%
  when 80..89 then 'val'    # 10%
  else 'test'                # 10%
end
```

**Benefits**:
- Same seed + same thread_id = same split (forever)
- No randomness = no accidental drift
- Quota enforcement at bucket level

### 2. Windowing for Mega-Threads

Long threads are sliced into **overlapping windows**:

```
Thread with 250 messages, window_size=100, overlap=10:
  window_0: messages 0-99
  window_1: messages 90-189   (10-message overlap)
  window_2: messages 180-249
```

**All windows inherit the thread's split assignment** - if `thread_xyz` is assigned to `train`, then `thread_xyz_window_0`, `thread_xyz_window_1`, etc. all go to train.

**Rationale**: Windows are overlapping "study slices" of one conversation, not independent examples.

Both --window-size and --window-overlap (options to splitter.rb) are fixed in the manifest creation within assignments.json;
therefore these **Window flags** are missing from retrain.rb, because retrain.rb merely consumes that manifest.


### 3. Sharding (Orthogonal Concept)

`--save-train-shards` in `retrain.rb` splits the **already-assigned train set** into multiple files for I/O performance:

```
train_shard_0.jsonl  (10k examples)
train_shard_1.jsonl  (10k examples)
...
```

**Not split assignment** - just packaging for faster loading/resuming. Use both windowing (logical slicing) and sharding (physical I/O) together.

---

## Tool Chain

### **mbox_pre-parser.rb**

Parses raw mbox into JSON with Message-Id, thread_id, subject, body, etc.:

```bash
mbox_pre-parser.rb my_archive.mbox > emails/intermediate.json
```

### **splitter.rb**

Assigns/appends IDs to the immutable manifest and outputs split directories:

```bash
splitter.rb \
  -i emails \
  -o split_output \
  -m assignments.json \
  --incremental \
  -s 42 \
  --window-size 100 \
  --window-overlap 10
```

**Flags**:
- `-i`: Input directory with intermediate.json
- `-o`: Output directory for split JSONLs
- `-m`: Manifest file (reused forever)
- `--incremental`: Only process new IDs not in manifest
- `-s`: Hash seed (never change after first run!)
- `--window-size/--window-overlap`: Chunking for mega-threads

**On first run**: Creates `assignments.json` and assigns all IDs  
**On retrain**: Reads existing manifest, only appends new IDs with same deterministic logic

### **immutable_manifest.rb**

Utility for manifest operations:

```bash
# Add a single ID manually
immutable_manifest.rb assign -m assignments.json -i msg@example.com -t thread_abc -s 42

# Materialize split lists from manifest
immutable_manifest.rb materialize -m assignments.json -o split_output

# Inspect manifest stats
immutable_manifest.rb stats -m assignments.json
```

**Materialize** reads the manifest and generates:
```
split_output/train.jsonl
split_output/val.jsonl
split_output/test.jsonl
```

### **retrain.rb** (with shard_window_manager.rb)

CPT training script with rolling-window incremental learning:

```bash
retrain.rb \
  --train splits/train.jsonl \
  --val splits/val.jsonl \
  --save-train-shard data/shards \
  --base-model model_v1.pth
  --keep-shards 8
```

**Sampler.rb** in the CI Workflow Pipeline

#### Overview

`sampler.rb` (StratifiedReplaySampler) implements stratified rehearsal sampling to prevent catastrophic forgetting during incremental CPT training. It ensures **rare senders and threads** get minimum representation in each epoch's rehearsal quota, while maintaining global proportionality.

---

#### What It Does

**StratifiedReplaySampler** takes **old training data** (from previous shards), builds hierarchical buckets (sender → thread → samples), and samples a stratified rehearsal batch that:

1. **Guarantees minimum representation** for each sender/thread bucket (default: 2 samples/bucket).
2. **Fills remaining quota proportionally** by sender frequency and thread weight.
3. **Interleaves rehearsal batches with new data** at a configurable ratio (default: 1:4 replay:new).

---

#### Key Class API

```ruby
require 'sampler'

# Initialize with old training data (JSON array of {sender:, thread_id:, ...})
sampler = StratifiedReplaySampler.new(
  old_train_data,              # Array of prior training examples
  min_per_bucket: 2            # Minimum samples per sender/thread bucket
)

# Sample n rehearsal examples with stratified weighting
rehearsal_batch = sampler.sample(n)  # Returns Array of n samples
```

---

#### Integration in CI Workflow

##### **1. Invocation Point**

`sampler.rb` is invoked **before** `retrain.rb` to prepare a **replay manifest** for the current training window:

```bash
# Step 1: Generate stratified replay batch for window_idx=3
ruby lib/sampler.rb \
  data/old_shards/shard_0_2_train.json \
  data/shard_3_train.json \
  --replay-ratio 1:4 \
  --batch-size 16 \
  --min-per-bucket 2 \
  --output data/shard_3_replay_manifest.json
```

**Inputs:**
- `OLD_TRAIN.json` — Consolidated train-split from previous shards (0–N-1).
- `NEW_TRAIN.json` — Current window's train-split examples (shard N).
- `--replay-ratio` — Rehearsal:new interleave ratio (e.g., 1:4 = 20% rehearsal).
- `--batch-size` — Batch size for interleaving calculation.
- `--min-per-bucket` — Minimum samples per sender/thread to avoid rare-bucket erasure.
- `--output` — Path to write the interleaved training manifest (rehearsal + new, shuffled).

**Outputs:**
- `shard_3_replay_manifest.json` — Interleaved batch manifest (new + stratified replay, shuffled).

---

##### **2. CI Workflow Integration**

A CI workflow pipeline is the automated build-test-lint-and-often-deploy sequence that runs on code
changes.  It used YAML because YAML is a human-readable, declarative, versioned config that cleanly 
describes jobs, and dependencies.  The CI service parses the YAML, starts runners (VM/containers), 
injects secrets, restores caches, and executes the directed acyclic graphs (DAG) with concurrency rules,
uploads artifacts, posts logs and status checks, and optionally deploys.

**Full pipeline sequence:**

```yaml
# .github/workflows/cpt_train.yml
steps:
  - name: Load assignments.json
    run: |
      # assignments.json: frozen split assignments (id → {split, thread_id, window_idx})
      # Determines which thread-windows belong to train/val/test per window_idx.
  
  - name: Extract current window train-split
    run: |
      # Parse assignments.json to extract train-split IDs for current window_idx=N
      ruby scripts/extract_split.rb \
        --assignments data/assignments.json \
        --window-idx $WINDOW_IDX \
        --split train \
        --output data/shard_${WINDOW_IDX}_train.json
  
  - name: Consolidate old shards (0–N-1)
    run: |
      # Merge all prior train-split shards into one consolidated file
      cat data/shard_0_train.json ... data/shard_$((WINDOW_IDX-1))_train.json \
        | jq -s 'add' > data/old_shards_train.json
  
  - name: Generate stratified replay manifest
    run: |
      ruby lib/sampler.rb \
        data/old_shards_train.json \
        data/shard_${WINDOW_IDX}_train.json \
        --replay-ratio 1:4 \
        --batch-size 16 \
        --min-per-bucket 2 \
        --output data/shard_${WINDOW_IDX}_replay_manifest.json
  
  - name: Train with replay manifest
    run: |
      ruby retrain.rb \
        --model-path models/checkpoint_window_$((WINDOW_IDX-1)) \
        --train-manifest data/shard_${WINDOW_IDX}_replay_manifest.json \
        --val-split data/shard_${WINDOW_IDX}_val.json \
        --output-path models/checkpoint_window_${WINDOW_IDX} \
        --epochs 3 \
        --lr 5e-5 \
        --batch-size 16
  
  - name: Upload checkpoint + replay manifest
    uses: actions/upload-artifact@v3
    with:
      name: checkpoint_window_${WINDOW_IDX}
      path: |
        models/checkpoint_window_${WINDOW_IDX}/
        data/shard_${WINDOW_IDX}_replay_manifest.json
```

---

##### **3. Deterministic Seeding**

To ensure **reproducible rehearsal sampling** across CI runs:

```ruby
# In sampler.rb, add seed parameter:
def initialize(old_train_data, min_per_bucket: 2, seed: 42)
  @seed = seed
  Random.srand(@seed)  # Set global seed
  # ... rest of initialization
end

def sample(n)
  rng = Random.new(@seed)  # Use seeded RNG for reproducibility
  # Replace .sample(take) with .sample(take, random: rng)
end
```

**CLI flag:**
```bash
ruby sampler.rb ... --seed 12345
```

---

##### **4. Caching & Artifacts**


**Cache old_shards_train.json** to avoid re-parsing assignments.json every run:

```yaml
- name: Cache old shards
  uses: actions/cache@v3
  with:
    path: data/old_shards_train.json
    key: old-shards-${{ env.WINDOW_IDX }}
```

**Upload replay manifests** as CI artifacts for audit/debug:

```yaml
- name: Upload replay manifest
  uses: actions/upload-artifact@v3
  with:
    name: replay_manifest_window_${{ env.WINDOW_IDX }}
    path: data/shard_${WINDOW_IDX}_replay_manifest.json
```

---

#### Rationale

**Why Stratified Sampling?**
- **Prevents rare-sender erasure:** Ensures low-volume senders/threads get minimum rehearsal slots.
- **Maintains global distribution:** Proportional sampling respects overall sender frequency.
- **Catastrophic forgetting mitigation:** Interleaving old examples during incremental training prevents model collapse on new data.

**Why Pre-generate Manifest?**
- **Decouples sampling from training:** Sampler.rb runs once per window, output is cached/auditable.
- **Enables batch-level control:** Retrain.rb consumes a static manifest, no runtime randomness.
- **Reproducibility:** Seeded sampling + frozen manifest = deterministic training pipeline.

---

#### Alternative: Inline Sampler in retrain.rb

If you prefer **on-the-fly sampling** instead of pre-generated manifests:

```ruby
# In retrain.rb:
require 'sampler'

old_train = JSON.parse(File.read('data/old_shards_train.json'))
new_train = JSON.parse(File.read("data/shard_#{window_idx}_train.json"))

sampler = StratifiedReplaySampler.new(old_train, min_per_bucket: 2, seed: 42)

# Interleave during epoch loop:
new_train.each_slice(batch_size * 4) do |new_chunk|
  replay_batch = sampler.sample((new_chunk.size * 0.25).ceil)
  combined = (new_chunk + replay_batch).shuffle(random: Random.new(42))
  train_batch(combined)
end
```

**Trade-off:**
- ✅ Simpler pipeline (no separate sampler.rb step).
- ❌ Harder to audit/cache rehearsal selection.
- ❌ Seeding must be managed inside retrain.rb.

---

#### Summary

| **Component**       | **Role**                                                                 |
|---------------------|--------------------------------------------------------------------------|
| `sampler.rb`        | Stratified rehearsal sampler (sender/thread buckets + proportional fill) |
| **Inputs**          | `old_shards_train.json` + `shard_N_train.json` + ratio/batch/seed        |
| **Outputs**         | `shard_N_replay_manifest.json` (interleaved new + rehearsal, shuffled)   |
| **CI Integration**  | Step 3 in CI pipeline: generate manifest → pass to retrain.rb            |
| **Reproducibility** | Seeded RNG + frozen manifest = deterministic rehearsal across runs       |
| **Anti-forgetting** | Rare buckets get min representation, common buckets fill proportionally  |

---
RNG is a pseudorandom number generator initalized with a specific seed so the "random" sequence is
deterministic and repeatable.  The same seeds leads to the same shuffles/samples.  different seeds 
cause different but reproducible runs.  So we log the seed and don't use nondeterministic operations
that can break bitwise repeatability.

---

## End-to-End Workflow

### **Initial Training**

1. **Parse mbox**:
   ```bash
   mbox_pre-parser.rb archive_v1.mbox > emails/intermediate.json
   ```

2. **Split with manifest creation**:
   ```bash
   splitter.rb -i emails -o splits -m assignments.json --incremental -s 42 \
     --window-size 100 --window-overlap 10
   ```
   ➜ Creates `assignments.json` with frozen assignments

3. **Materialize splits**:
   ```bash
   immutable_manifest.rb materialize -m assignments.json -o splits
   ```
   ➜ Generates `splits/{train,val,test}.jsonl`

4. **Train model**:
   ```bash
   retrain.rb --train splits/train.jsonl --val splits/val.jsonl \
     --save-train-shard data/shards base-model model_v1.pth
   ```

### **Incremental Retrain (New Emails Arrive)**

1. **Parse new mbox**:
   ```bash
   Mbox_pre-parser.rb archive_v2.mbox > emails_new/intermediate.json
   ```

2. **Re-run splitter with same manifest and seed**:
   ```bash
   splitter.rb -i emails_new -o splits -m assignments.json --incremental -s 42 \
     --window-size 100 --window-overlap 10
   ```
   ➜ **Only appends new IDs** to `assignments.json`; existing entries untouched

3. **Re-materialize**:
   ```bash
   immutable_manifest.rb materialize -m assignments.json -o splits
   ```
   ➜ Updates splits with new data, but old assignments stay frozen

4. **Continue training**:
   ```bash
   retrain.rb --train splits/train.jsonl --val splits/val.jsonl \
     --base-model model_v1.pth --save-train-shard data/shards
   ```

---

## Design Guarantees

✅ **Reproducibility**: Same manifest + same seed = identical splits across runs  
✅ **Immutability**: Test set never contaminates train set in future retrains  
✅ **Thread integrity**: All messages/windows in a thread share the same split  
✅ **Incrementality**: New data appends deterministically without reshuffling old data  
✅ **Scalability**: Windowing handles mega-threads; sharding handles I/O

---

## Key Design Decisions

### Why hash-bucketing instead of random sampling?
**Determinism**: Random splits are non-reproducible without serializing the entire RNG state. Hash-bucketing guarantees the same thread always lands in the same bucket given the same seed.

### Why append-only manifest?
**Frozen reproducibility**: Never mutate existing assignments. Scientific experiments require fixed test sets. Adding new data shouldn't change how we evaluate old performance.

### Why thread-level assignment?
**Leakage prevention**: If message A and message B are in the same conversation, training on A and testing on B violates independence. Thread-level assignment prevents this.

### Why do windows inherit thread splits?
**Same reason**: Windows are slices of one conversation, albeit overlapping ones. Training on `thread_xyz_window_0` and testing on `thread_xyz_window_1` is leakage.

### Why two-layer design (manifest + materialize)?
**Efficiency**: Manifest is a compact map; materialization generates full splits on demand. Supports multiple export formats (JSONL, CSV, etc.) from one source of truth.

---

## Troubleshooting

**Q: I changed the seed and now my splits are different!**  
A: Don't do that. The seed is part of the experiment signature. Changing it invalidates reproducibility.

**Q: Can I manually move an ID from train to test?**  
A: You *can* edit `assignments.json`, but you're breaking immutability. Only do this if you have a very good reason (e.g., discovered PII in test set).

**Q: What if I delete old emails from the archive?**  
A: The manifest retains their assignments. If you re-materialize, those IDs won't appear in the output (no source data), but the manifest preserves history.

**Q: How do I reset everything and start fresh?**  
A: Delete `assignments.json` and re-run with a new seed. All assignments will be recomputed.

---

## Summary

This architecture provides **production-grade ML hygiene** for email CPT:

1. **Parse** → intermediate JSON
2. **Split** → immutable manifest (append-only, deterministic, thread-aware)
3. **Materialize** → train/val/test JSONLs
4. **Train** → CPT with sharded I/O

**The golden rule**: Treat `assignments.json` as **permanent marker**. Once written, never erase. Only append.

**Result**: Scientifically reproducible, incrementally trainable, leakage-proof email AI.

# The following was added by David Roderick


## sampler.rb
Step 4: Create a mixed group of 20% replayed email threads with 80% new threads from batches

sampler.rb builds the actual training mix.  It takes the historical mixed emails (old_train.json) and the fresh batch
(new_train.json) and samples them with the requested ratio (e.g. 1:4 = one part old to 4 prts new), using the stratified 
replay rules and writes "mixed_train.json", which is exactly what we pass to retrain.rb on a run of lora_checkpoint_selector.rb 


# Prior to RAG
```bash
jq -c '.[]' train.json > train.jsonl
jq -c '.[]' val.json > val.jsonl
jq -c '.[]' test.json > test.jsonl

cat train.jsonl val.jsonl test.jsonl > Rag_train.jsonl 
```
The advantage of jsonl for weaviate database is that JSONL lets you stream and append with one object per line, 
cat/pipe/parallelize shards trivially

# Plan for RAG
1. Build a RAG baseline (Postgres+pgvector)
2. train LoRA on Colab with retrain.rb with train.jsonl using --early-stop --old-train old_train.json



